# High Availability Docker Compose Configuration
# Multi-instance deployment with load balancing and clustering

services:
  # Load Balancer (HAProxy)
  load-balancer:
    image: haproxy:2.8
    container_name: telemetry-lb
    ports:
      - "80:80"          # HTTP load balancer
      - "8404:8404"      # HAProxy stats
      - "3000:3000"      # Grafana (load balanced)
      - "8086:8086"      # InfluxDB (load balanced)
    volumes:
      - ./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    depends_on:
      - influxdb-1
      - influxdb-2
      - grafana-1
      - grafana-2
    networks:
      - telemetry-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      interval: 30s
      timeout: 10s
      retries: 3

  # InfluxDB Cluster (2 nodes)
  influxdb-1:
    image: influxdb:2.7
    container_name: influxdb-primary
    ports:
      - "8087:8086"
    volumes:
      - influxdb1_data:/var/lib/influxdb2
      - influxdb1_config:/etc/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUXDB_PASSWORD:-admin123!}
      - DOCKER_INFLUXDB_INIT_ORG=nflx
      - DOCKER_INFLUXDB_INIT_BUCKET=default
      - INFLUXD_HTTP_BIND_ADDRESS=:8086
    networks:
      - telemetry-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/ping"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

  influxdb-2:
    image: influxdb:2.7
    container_name: influxdb-secondary
    ports:
      - "8088:8086"
    volumes:
      - influxdb2_data:/var/lib/influxdb2
      - influxdb2_config:/etc/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUXDB_PASSWORD:-admin123!}
      - DOCKER_INFLUXDB_INIT_ORG=nflx
      - DOCKER_INFLUXDB_INIT_BUCKET=default
      - INFLUXD_HTTP_BIND_ADDRESS=:8086
    networks:
      - telemetry-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/ping"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Grafana Cluster (2 nodes with shared storage)
  grafana-1:
    image: grafana/grafana:latest
    container_name: grafana-primary
    ports:
      - "3001:3000"
    volumes:
      - grafana_shared_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ./grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD:-admin123!}
      - GF_SECURITY_SECRET_KEY=${GF_SECURITY_SECRET_KEY:-your-secret-key-here}
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel,grafana-piechart-panel
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres:5432
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=grafana
      - GF_DATABASE_PASSWORD=${POSTGRES_PASSWORD:-grafana123!}
      - INFLUXDB_TOKEN=${INFLUXDB_TOKEN:-}
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID:-}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET:-}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - telemetry-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  grafana-2:
    image: grafana/grafana:latest
    container_name: grafana-secondary
    ports:
      - "3002:3000"
    volumes:
      - grafana_shared_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ./grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD:-admin123!}
      - GF_SECURITY_SECRET_KEY=${GF_SECURITY_SECRET_KEY:-your-secret-key-here}
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel,grafana-piechart-panel
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres:5432
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=grafana
      - GF_DATABASE_PASSWORD=${POSTGRES_PASSWORD:-grafana123!}
      - INFLUXDB_TOKEN=${INFLUXDB_TOKEN:-}
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID:-}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET:-}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - telemetry-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Shared PostgreSQL database for Grafana clustering
  postgres:
    image: postgres:15
    container_name: grafana-postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=grafana
      - POSTGRES_USER=grafana
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-grafana123!}
    networks:
      - telemetry-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U grafana -d grafana"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Telemetry Service Cluster (3 instances)
  network-telemetry-1:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: telemetry-service-1
    environment: &telemetry-env
      - INSTANCE_ID=telemetry-1
      - TARGET_FQDN=${TARGET_FQDN:-google.com,github.com,cloudflare.com}
      - INFLUXDB_URL=http://load-balancer:8086
      - INFLUXDB_TOKEN=${INFLUXDB_TOKEN:-}
      - INFLUXDB_ORG=nflx
      - INFLUXDB_BUCKET=default
      - MONITORING_INTERVAL=${MONITORING_INTERVAL:-60}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Scaling configuration
      - MAX_CONNECTIONS=50
      - MAX_CONCURRENT_TARGETS=20
      - MAX_CONCURRENT_OPERATIONS=50
      - CIRCUIT_FAILURE_THRESHOLD=3
      - CIRCUIT_RECOVERY_TIMEOUT=30
      - BATCH_SIZE=5
      - WORKER_POOL_SIZE=10
    depends_on:
      load-balancer:
        condition: service_started
    networks:
      - telemetry-cluster
    restart: unless-stopped
    volumes:
      - ./logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://load-balancer:8086/ping', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3

  network-telemetry-2:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: telemetry-service-2
    environment:
      <<: *telemetry-env
      INSTANCE_ID: telemetry-2
    depends_on:
      load-balancer:
        condition: service_started
    networks:
      - telemetry-cluster
    restart: unless-stopped
    volumes:
      - ./logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://load-balancer:8086/ping', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3

  network-telemetry-3:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: telemetry-service-3
    environment:
      <<: *telemetry-env
      INSTANCE_ID: telemetry-3
    depends_on:
      load-balancer:
        condition: service_started
    networks:
      - telemetry-cluster
    restart: unless-stopped
    volumes:
      - ./logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://load-balancer:8086/ping', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis for caching and session storage
  redis:
    image: redis:7-alpine
    container_name: telemetry-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    networks:
      - telemetry-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: telemetry-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - telemetry-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # AlertManager for alerting
  alertmanager:
    image: prom/alertmanager:latest
    container_name: telemetry-alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    networks:
      - telemetry-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  telemetry-cluster:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/24

volumes:
  influxdb1_data:
    driver: local
  influxdb1_config:
    driver: local
  influxdb2_data:
    driver: local
  influxdb2_config:
    driver: local
  grafana_shared_data:
    driver: local
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  alertmanager_data:
    driver: local